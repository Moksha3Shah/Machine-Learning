{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import random\n",
    "import string\n",
    "import re, string, unicodedata\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import wikipedia as wk\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('punkt') \n",
    "nltk.download('wordnet')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import csv\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "import google.cloud\n",
    "from firebase_admin import credentials, firestore\n",
    "from firebase_admin import db\n",
    "\n",
    "if (not len(firebase_admin._apps)):\n",
    "    cred = credentials.Certificate(r\"C:\\Users\\ADMIN\\Documents\\Alorine\\alorinedotcom-firebase-adminsdk-ii0g7-d2bc0817d1.json\")\n",
    "    default_app = firebase_admin.initialize_app(cred)\n",
    "\n",
    "db = firestore.client()\n",
    "posts_table = db.collection(u'posts')\n",
    "res = rec = []\n",
    "google = raw = ''\n",
    "try:\n",
    "    posts_data = posts_table.stream()\n",
    "    for each_post in posts_data:\n",
    "        pst = each_post.to_dict()\n",
    "        list = [[k, v] for k, v in pst.items()]\n",
    "        output = [item for item in list if (item[0] == 'title' or item[0] == 'category' or item[0] == 'authorUid' or item[0] == 'hits')]\n",
    "        res = res + output\n",
    "except google.cloud.exceptions.NotFound:\n",
    "    print(u'Missing data')   \n",
    "\n",
    "def extract_key(v):\n",
    "    return v[0]\n",
    "res = sorted(res, key=extract_key)\n",
    "result = [[x[1] for x in g] for k, g in itertools.groupby(res, extract_key)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = q = r = s = t = u = []\n",
    "i = 0\n",
    "m = ''\n",
    "pattern = re.compile(r'(,\\s){2,}')\n",
    "try:\n",
    "    posts_data = posts_table.stream()\n",
    "    for each_post in posts_data:\n",
    "        pst = each_post.to_dict()\n",
    "        list = [[k, v] for k, v in pst.items()]\n",
    "        p = [item for item in list if (item[0] == 'bodyJson')]\n",
    "        q = q + p\n",
    "except google.cloud.exceptions.NotFound:\n",
    "    print(u'Missing data')\n",
    "r = [[x[1] for x in g] for k, g in itertools.groupby(q, extract_key)]    \n",
    "\n",
    "for lists in r:\n",
    "    lists = [i.strip(\"[]\").split(\", \") for i in lists] \n",
    "    s = s + lists\n",
    "\n",
    "for lists in s:\n",
    "    b = ''\n",
    "    for item in lists:\n",
    "        item = item.replace('{\"insert\":', ' ')\n",
    "        item = item.replace('\"attributes\":{\"block\":\"ul\"}}', ' ')\n",
    "        item = item.replace('\"attributes\":{\"block\":\"ol\"}}', ' ')\n",
    "        item = item.replace('\\\\n',' ')\n",
    "        item = re.sub(\"[^!,a-z'A-Z.]\", \" \", item)\n",
    "        item = re.sub(pattern, ' ', item)\n",
    "        item = re.sub(',+', ' ',item)\n",
    "        item = item.replace('attributes','')\n",
    "        item = re.sub(' +', ' ',item)\n",
    "        b = b + \" \" + item\n",
    "    b = b.lower()\n",
    "    t.insert(i, b)\n",
    "    m += b\n",
    "    i += 1\n",
    "\n",
    "result.append(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "export_data = zip_longest(*result, fillvalue = '')\n",
    "csvfile=open('data.csv', 'w', newline='', encoding=\"utf-8\")\n",
    "with open('ALORINE_data.csv', 'w', encoding=\"utf-8\", newline='') as myfile:\n",
    "      wr = csv.writer(myfile, delimiter=',')\n",
    "      wr.writerow((\"title\", \"category\", \"authorUid\", \"hits\", \"bodyJson\"))\n",
    "      wr.writerows(export_data)\n",
    "        \n",
    "myfile.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = m.lower()\n",
    "sent_tokens = nltk.sent_tokenize(raw)\n",
    "\n",
    "def Normalize(text):\n",
    "    remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "    #word tokenization\n",
    "    word_token = nltk.word_tokenize(text.lower().translate(remove_punct_dict))\n",
    "    \n",
    "    #remove ascii\n",
    "    new_words = []\n",
    "    for word in word_token:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    \n",
    "    #Remove tags\n",
    "    rmv = []\n",
    "    for w in new_words:\n",
    "        text=re.sub(\"&lt;/?.*?&gt;\",\"&lt;&gt;\",w)\n",
    "        rmv.append(text)\n",
    "        \n",
    "    #pos tagging and lemmatization\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    lemma_list = []\n",
    "    rmv = [i for i in rmv if i]\n",
    "    for token, tag in nltk.pos_tag(rmv):\n",
    "        lemma = lmtzr.lemmatize(token, tag_map[tag[0]])\n",
    "        lemma_list.append(lemma)\n",
    "    return lemma_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "welcome_input = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "welcome_response = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "def welcome(user_response):\n",
    "    for word in user_response.split():\n",
    "        if word.lower() in welcome_input:\n",
    "            return random.choice(welcome_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generateResponse(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=Normalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    #vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    vals = linear_kernel(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0) or \"tell me about\" in user_response:\n",
    "        print(\"Just a moment...\")\n",
    "        if user_response:\n",
    "            robo_response = wikipedia_data(user_response)\n",
    "            return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response\n",
    "#wikipedia search\n",
    "def wikipedia_data(input):\n",
    "    reg_ex = re.search('tell me about (.*)', input)\n",
    "    try:\n",
    "        if reg_ex:\n",
    "            topic = reg_ex.group(1)\n",
    "            wiki = wk.summary(topic, sentences = 3)\n",
    "            return wiki\n",
    "    except Exception as e:\n",
    "            print(\"I apologise! I don't understand.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendation(rec):\n",
    "    if isinstance(rec, str) == True:\n",
    "        with open('ALORINE_data.csv', 'r', encoding=\"utf-8\") as file:\n",
    "            reader = csv.reader(file, delimiter=',')\n",
    "            for row in reader:\n",
    "                if rec in row[4]:\n",
    "                    print(\"Recommendation: \"+row[3])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I am Alina and I'm here to answer your queries. If you want to exit, type Bye!\n",
      "Hello\n",
      "Alina : hi\n",
      "Tell me about Health\n",
      "Alina : Just a moment...\n",
      "Health is a state of physical, mental and social well-being in which disease and infirmity are absent.\n",
      "\n",
      "\n",
      "== History ==\n",
      "The meaning of health has evolved over time. In keeping with the biomedical perspective, early definitions of health focused on the theme of the body's ability to function; health was seen as a state of normal function that could be disrupted from time to time by disease.\n",
      "what are the popular dieting myths?\n",
      "Alina : here are the most popular diet myths night time eating makes you fat night and day have nothing to do with your weight gain.\n",
      "Recommendation: 3 Popular dieting myths\n",
      "Thank you\n",
      "Alina : You are welcome..\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"Hello! I am Alina and I'm here to answer your queries. If you want to exit, type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response not in ['bye','shutdown','exit', 'quit']):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"Alina : You are welcome..\")\n",
    "        else:\n",
    "            if(welcome(user_response)!=None):\n",
    "                print(\"Alina : \"+welcome(user_response))\n",
    "            else:\n",
    "                print(\"Alina : \",end=\"\")\n",
    "                rec = generateResponse(user_response)\n",
    "                print(rec)\n",
    "                sent_tokens.remove(user_response)\n",
    "                recommendation(rec)             \n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"Alina : Bye!!! \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
